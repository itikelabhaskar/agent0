# AgentX Setup - BigQuery + Dataplex Integration

## ğŸ¯ Quick Setup (3 Commands)

```powershell
# 1. Authenticate to GCP
gcloud auth application-default login

# 2. Run complete setup
python setup_bigquery_dataplex.py

# 3. Start the application
python run_backend.py              # Terminal 1
streamlit run frontend/app.py       # Terminal 2
```

That's it! Your data is in BigQuery, Dataplex is profiling it, and agents are ready.

---

## ğŸ“š What This Does

### âœ… Correct Architecture (As Per Problem Statement)

```
Excel File
    â†“
Direct Upload to BigQuery
    â†“
Dataplex Profiling (Automated)
    â”œâ†’ Column statistics
    â”œâ†’ Null ratios & completeness
    â”œâ†’ Value distributions
    â””â†’ Auto-generated DQ rules
    â†“
Identifier Agent (sits on top of Dataplex)
    â”œâ†’ Uses Dataplex profiles
    â”œâ†’ Queries BigQuery
    â”œâ†’ Enables business users to create custom rules via NL
    â””â†’ Detects data quality issues
    â†“
Treatment Agent
    â””â†’ Analyzes patterns in BigQuery
    â†“
Remediator Agent
    â””â†’ Fixes data in BigQuery
```

### âŒ What We DON'T Do

- âŒ Store CSV files locally
- âŒ Read from local files
- âŒ Have hardcoded test data
- âŒ Use file-based fallbacks

### âœ… What We DO

- âœ… Upload directly to BigQuery (cloud-native)
- âœ… Use Dataplex for automated profiling
- âœ… Agents query BigQuery only
- âœ… Identifier sits on top of Dataplex (as per problem statement)
- âœ… Production-ready architecture

---

## ğŸ“Š Data Overview

**Source**: `actualdata/1.1 Improving IP& Data Quality_BaNCs Synthetic Data - DQM AI Use Case.xlsx`

**Tables Created in BigQuery**:
- `week1` - ~100 records (Week 1 snapshot)
- `week2` - ~100 records (Week 2 snapshot)
- `week3` - ~100 records (Week 3 snapshot)
- `week4` - ~100 records (Week 4 snapshot)

**System Tables**:
- `rules` - Data quality rules
- `issues` - Detected issues
- `users` - User management (RBAC)
- `audit_log` - Action tracking

**Schema** (27 fields per record):
- Customer: `CUS_ID`, `CUS_FORNAME`, `CUS_SURNAME`, `CUS_DOB`, `CUS_NI_NO`, `CUS_POSTCODE`
- Life Status: `CUS_LIFE_STATUS`, `CUS_DEATH_DATE`, `CUS_SMOKER_STAT`
- Scheme: `SCM_MEMBER_STATUS`, `SCH_SCHEME_TYP`, `SCH_RENEWAL_DT`
- Payments: `POLI_GROSS_PMT`, `POLI_TAX_PMT`, `UNT_TRAN_AMT`
- ... and more

---

## ğŸ› Known Data Quality Issues (Planted for Testing)

1. **Missing DOB** (~15-20 records): `CUS_DOB` is NULL
2. **Missing Postcode** (several records): `CUS_POSTCODE` is empty
3. **Invalid Dates**: "31/11/1997", "30/02/2007" (impossible dates)
4. **Deceased with Active Policies**: `CUS_LIFE_STATUS='DEC'` but `SCM_MEMBER_STATUS='Active'`
5. **Negative Payments**: Some `POLI_GROSS_PMT < 0`
6. **Payment Outliers**: Detected via Dataplex Z-score statistics

---

## ğŸ” How Dataplex Integration Works

### Identifier Agent Uses Dataplex

```python
from agent.dataplex_integration import dataplex

# 1. Get automated rule suggestions from Dataplex profile
rules = dataplex.suggest_rules_from_profile("week1")
# Returns:
# - Completeness rules (high null ratios)
# - Accuracy rules (outliers via IQR)
# - Validity rules (string length anomalies)

# 2. Get data quality scores
dq_score = dataplex.calculate_dq_score_from_profile("week1")
# Returns:
# - Completeness score
# - Consistency score
# - Overall DQ score

# 3. Query BigQuery for actual issues
from agent.tools import run_bq_query
issues = run_bq_query(PROJECT, "SELECT * FROM week1 WHERE CUS_DOB IS NULL")
```

### Benefits of Dataplex Integration

1. **Automated Profiling**: Dataplex scans data and provides statistics
2. **Rule Generation**: Identifier generates rules based on profile insights
3. **Business User Friendly**: NL â†’ SQL on top of Dataplex profiles
4. **Scalable**: Works with any BigQuery table size
5. **Production Ready**: Google-managed service

---

## ğŸ§ª Testing the Setup

### 1. Verify Tables Exist

```powershell
bq ls hackathon-practice-480508:dev_dataset
```

Expected output:
```
week1
week2
week3
week4
rules
issues
users
audit_log
```

### 2. Check Data

```powershell
bq query "SELECT COUNT(*) as count FROM \`hackathon-practice-480508.dev_dataset.week1\`"
```

Expected: ~100 rows

### 3. View Dataplex Profiles

Visit: https://console.cloud.google.com/dataplex/process/data-scans?project=hackathon-practice-480508

You should see:
- `profile_week1`
- `profile_week2`
- `profile_week3`
- `profile_week4`

Wait 2-5 minutes for first profiles to complete.

### 4. Test Identifier Agent

1. Open frontend: http://localhost:8501
2. Select "dev_dataset.week1"
3. Click "Run Identifier"
4. Should detect ~15-20 customers with missing DOB

### 5. Test NL â†’ SQL with Dataplex

In frontend, go to "NL â†’ SQL" section:
- Enter: "Find customers with missing date of birth"
- Click "Generate SQL"
- Should generate: `SELECT CUS_ID, CUS_FORNAME, CUS_SURNAME, CUS_DOB FROM week1 WHERE CUS_DOB IS NULL`

---

## ğŸ”§ Troubleshooting

### "Permission denied"
```powershell
gcloud auth application-default login
```

### "Dataplex not available"
```powershell
pip install google-cloud-dataplex
```

### "Table not found"
Re-run setup:
```powershell
python setup_bigquery_dataplex.py
```

### "No Dataplex profiles"
Wait 2-5 minutes after setup, then check:
https://console.cloud.google.com/dataplex/process/data-scans?project=hackathon-practice-480508

---

## ğŸ“ Project Structure (After Setup)

```
agent0/
â”œâ”€â”€ actualdata/
â”‚   â””â”€â”€ 1.1 Improving...xlsx          # Source data (stays here)
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ identifier.py                  # Uses Dataplex + BigQuery
â”‚   â”œâ”€â”€ treatment.py                   # Analyzes BigQuery patterns
â”‚   â”œâ”€â”€ remediator.py                  # Fixes BigQuery data
â”‚   â””â”€â”€ dataplex_integration.py        # Dataplex client
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ main.py                        # FastAPI with BQ + Dataplex endpoints
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py                         # Streamlit UI
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ upload_excel_to_bigquery.py    # Direct Excel â†’ BQ
â”‚   â””â”€â”€ setup_dataplex.py              # Dataplex profiling setup
â”œâ”€â”€ setup_bigquery_dataplex.py         # â­ ONE-COMMAND SETUP
â”œâ”€â”€ DIRECT_BQ_SETUP.md                 # Detailed guide
â””â”€â”€ README_SETUP.md                    # This file
```

---

## ğŸ“ Key Differences from Wrong Approach

| Aspect | âŒ Wrong | âœ… Correct |
|--------|---------|-----------|
| **Data Storage** | Local CSV files | BigQuery tables |
| **Agent Queries** | Read CSV files | Query BigQuery |
| **DQ Rules** | Hardcoded in code | Dataplex profiles + NLâ†’SQL |
| **Architecture** | File-based | Cloud-native |
| **Scalability** | Limited to local files | Unlimited (BigQuery) |
| **Identifier Role** | Standalone agent | Sits on top of Dataplex |

---

## ğŸš€ Next Steps After Setup

1. âœ… Data in BigQuery
2. âœ… Dataplex profiling
3. âœ… Agents configured
4. **TODO**: Run end-to-end test
5. **TODO**: Generate rules for all 4 weeks
6. **TODO**: Track DQ improvement across weeks
7. **TODO**: Prepare hackathon demo

---

## ğŸ“ Quick Reference

- **BigQuery Console**: https://console.cloud.google.com/bigquery?project=hackathon-practice-480508
- **Dataplex Console**: https://console.cloud.google.com/dataplex?project=hackathon-practice-480508
- **Frontend**: http://localhost:8501 (after starting)
- **Backend API**: http://localhost:8080 (after starting)

---

**Ready? Run:** `python setup_bigquery_dataplex.py` ğŸš€

