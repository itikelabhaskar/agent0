# AgentX Setup Guide

## ğŸ“‹ Configuration

- **Project**: `prod-12-335`
- **Dataset**: `dev_dataset`
- **GCS Bucket**: `gs://prod-45-hackathon-bucket`
- **Data Folder**: `1.1 Improving IP& Data Quality/`
- **CSV Files**: `sbox-Week1.csv`, `sbox-Week2.csv`, `sbox-Week3.csv`, `sbox-Week4.csv`

## ğŸš€ Quick Start (2 Steps)

### Step 1: Verify Access

```powershell
python verify_gcs_setup.py
```

**This checks:**
- âœ… GCP authentication
- âœ… Access to bucket `prod-45-hackathon-bucket`
- âœ… CSV files exist in folder
- âœ… CSV data is valid
- âœ… BigQuery dataset status

**Expected output:**
```
âœ… Authenticated to GCP
   Project detected: prod-12-335

âœ… Bucket found: prod-45-hackathon-bucket

âœ… sbox-Week1.csv: 249.7 KB
âœ… sbox-Week2.csv: 249.8 KB
âœ… sbox-Week3.csv: 249.7 KB
âœ… sbox-Week4.csv: 249.7 KB

âœ… CSV valid: 100 rows, 27 columns

Configuration:
   â€¢ Project: prod-12-335
   â€¢ Dataset: dev_dataset
   â€¢ GCS Bucket: gs://prod-45-hackathon-bucket
   â€¢ CSV Files: sbox-Week1.csv, sbox-Week2.csv, sbox-Week3.csv, sbox-Week4.csv

Data will be loaded to:
   â€¢ prod-12-335.dev_dataset.week1
   â€¢ prod-12-335.dev_dataset.week2
   â€¢ prod-12-335.dev_dataset.week3
   â€¢ prod-12-335.dev_dataset.week4

Status: âš ï¸  Data not loaded yet
Next step: python load_from_gcs.py
```

### Step 2: Load Data

```powershell
python load_from_gcs.py
```

**This will:**
1. Read CSV files from GCS
2. Create BigQuery dataset `dev_dataset`
3. Load data to tables: `week1`, `week2`, `week3`, `week4`
4. Create auxiliary tables: `rules`, `issues`, `users`, `audit_log`

**Expected output:**
```
ğŸš€ Loading Data from GCS to BigQuery
======================================================================

Step 1/3: Reading CSV files from GCS...
   Reading sbox-Week1.csv...
      âœ… 100 rows, 27 columns
   Reading sbox-Week2.csv...
      âœ… 100 rows, 27 columns
   Reading sbox-Week3.csv...
      âœ… 100 rows, 27 columns
   Reading sbox-Week4.csv...
      âœ… 100 rows, 27 columns
âœ… All CSV files loaded

Step 2/3: Setting up BigQuery dataset...
âœ… Created dataset: dev_dataset

Step 3/3: Loading CSV data to BigQuery...
Loading â†’ week1...
   âœ… Loaded 100 rows to week1
Loading â†’ week2...
   âœ… Loaded 100 rows to week2
Loading â†’ week3...
   âœ… Loaded 100 rows to week3
Loading â†’ week4...
   âœ… Loaded 100 rows to week4

Creating auxiliary tables...
âœ… rules table ready
âœ… issues table ready
âœ… users table ready
âœ… audit_log table ready

======================================================================
âœ… LOAD COMPLETE!
======================================================================

Tables created in prod-12-335.dev_dataset:
   â€¢ week1: 100 rows
   â€¢ week2: 100 rows
   â€¢ week3: 100 rows
   â€¢ week4: 100 rows
   â€¢ rules: 0 rows
   â€¢ issues: 0 rows
   â€¢ users: 0 rows
   â€¢ audit_log: 0 rows
```

### Step 3: Start Application

```powershell
# Terminal 1: Backend
python run_backend.py

# Terminal 2: Frontend
streamlit run frontend/app.py
```

Open: http://localhost:8501

---

## ğŸ” Verification Commands

### Check bucket access:
```powershell
gsutil ls gs://prod-45-hackathon-bucket/
```

### Check CSV files:
```powershell
gsutil ls "gs://prod-45-hackathon-bucket/1.1 Improving IP& Data Quality/"
```

### Check BigQuery tables:
```powershell
bq ls prod-12-335:dev_dataset
```

### Query data:
```powershell
bq query --use_legacy_sql=false "SELECT COUNT(*) FROM \`prod-12-335.dev_dataset.week1\`"
```

### Sample data:
```powershell
bq query --use_legacy_sql=false "SELECT CUS_ID, CUS_FORNAME, CUS_SURNAME, CUS_DOB FROM \`prod-12-335.dev_dataset.week1\` LIMIT 5"
```

---

## ğŸ› Troubleshooting

### Error: "does not have storage.buckets.get access"

**Solution**: Authenticate to project with access:
```powershell
gcloud auth application-default login --project=prod-12-335
```

### Error: "File not found"

**Check files exist:**
```powershell
gsutil ls "gs://prod-45-hackathon-bucket/1.1 Improving IP& Data Quality/"
```

### Error: "Permission denied"

**Required roles:**
- Storage Object Viewer
- BigQuery Data Editor
- BigQuery Job User

---

## ğŸ“Š Data Schema

Each CSV file has **27 columns**:

**Customer Info:**
- CUS_ID, CUS_KEY_PARTY_ID, CUS_KEY_CUST_NO
- CUS_FORNAME, CUS_SURNAME, CUS_NI_NO
- CUS_DOB, CUS_SEX_CD, CUS_OCCUP_CD
- CUS_LIFE_STATUS, CUS_POSTCODE, CUS_SMOKER_STAT, CUS_DEATH_DATE

**Scheme Info:**
- CRL_KEY_POLICY_NO, SCM_PROJ_RET_DT, SCM_PROJ_RET_AGE
- SCM_SCH_LEAVE_DATE, SCM_MEMBER_STATUS
- SCH_SCHEME_TYP, SCH_RENEWAL_DT

**Payment Info:**
- POLID_FREQ, POLID_INCOME_TYPE, POLID_PAYMENT_DAY
- POLI_GROSS_PMT, POLI_TAX_PMT, POLI_INCOME_PMT
- UNT_TRAN_AMT

**~100 rows per week** = **~400 total records**

---

## âœ… Success Criteria

Setup is complete when:

1. âœ… `python verify_gcs_setup.py` passes all checks
2. âœ… `python load_from_gcs.py` loads 4 tables with 100 rows each
3. âœ… `bq ls prod-12-335:dev_dataset` shows 8 tables
4. âœ… Backend starts: `python run_backend.py`
5. âœ… Frontend loads: `streamlit run frontend/app.py`
6. âœ… Can select `dev_dataset.week1` and run identifier

---

## ğŸ“ What Gets Created

```
BigQuery:
prod-12-335
â””â”€â”€ dev_dataset
    â”œâ”€â”€ week1 (100 rows - sbox-Week1.csv)
    â”œâ”€â”€ week2 (100 rows - sbox-Week2.csv)
    â”œâ”€â”€ week3 (100 rows - sbox-Week3.csv)
    â”œâ”€â”€ week4 (100 rows - sbox-Week4.csv)
    â”œâ”€â”€ rules (system table)
    â”œâ”€â”€ issues (system table)
    â”œâ”€â”€ users (system table)
    â””â”€â”€ audit_log (system table)

GCS (source data):
gs://prod-45-hackathon-bucket/
â””â”€â”€ 1.1 Improving IP& Data Quality/
    â”œâ”€â”€ sbox-Week1.csv
    â”œâ”€â”€ sbox-Week2.csv
    â”œâ”€â”€ sbox-Week3.csv
    â””â”€â”€ sbox-Week4.csv
```

---

## ğŸ¯ Summary

**Two commands to get started:**
```powershell
python verify_gcs_setup.py    # Check access
python load_from_gcs.py         # Load data
```

**Then start the app:**
```powershell
python run_backend.py           # Terminal 1
streamlit run frontend/app.py   # Terminal 2
```

That's it! ğŸ‰

